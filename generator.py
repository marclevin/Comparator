import logging
import os
from typing import Tuple

from dotenv import load_dotenv
from openai import OpenAI

from compare import validate_student_attempts

teacher_attempt_count = 0


def generate_ai_hint(problem_description: str, student_code: str, edit: str, goal_code: str) -> str:
    load_dotenv()
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    if client is None:
        raise Exception("OpenAI client not initialized.")
    return internal_generate_ai_hint(problem_description, student_code, edit, goal_code, client)


def internal_generate_ai_hint(problem_description: str, student_code: str, edit: str, goal_code: str,
                              client: OpenAI) -> str:
    global teacher_attempt_count
    if teacher_attempt_count > 5:
        raise Exception("Too many attempts to generate a hint, stopping.")
    teacher_attempt_count += 1
    filled_template = populate_teacher_template(problem_description, student_code, edit)

    teacher_interaction = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "user", "content": f"{filled_template}"},
        ],
    )
    hints = extract_hints(teacher_interaction.choices[0].message.content)
    if hints is None:
        raise Exception("No hints were generated by the AI.")
    long_form_hint, short_form_hint = hints
    # Log the hints
    logging.log(logging.INFO, f"Long-form hint: {long_form_hint}")
    logging.log(logging.INFO, f"Short-form hint: {short_form_hint}")

    filled_student_template = populate_student_template(long_form_hint, student_code)
    completions = []
    for _ in range(10):
        completion_student = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "user", "content": f"{filled_student_template}"},
            ],
        )
        completions.append(completion_student.choices[0].message.content)
    logging.log(logging.INFO, f"Student completions: {completions}")
    # Now to compare the completions to the goal code.
    average_score = validate_student_attempts(completions, goal_code)
    # We want a solution that is at least 80% similar to the goal code.
    if average_score < 0.8:
        # We will handle the recursive attempt here.
        return internal_generate_ai_hint(problem_description, student_code, edit, goal_code, client)
    return short_form_hint


def populate_teacher_template(problem_description, student_code, edit):
    teacher_template = ""
    try:
        with open("prompt/teacher.txt", "r") as f:
            teacher_template = f.read()
    except FileNotFoundError:
        raise Exception("Teacher template not found.")
    filled_template = teacher_template.format(
        problem_description=problem_description,
        student_code=student_code,
        edit=edit,
    )
    return filled_template


def populate_student_template(long_form_hint, student_code):
    student_template = ""
    try:
        with open("prompt/student.txt", "r") as f:
            student_template = f.read()
    except FileNotFoundError:
        raise Exception("Student template not found.")
    filled_student_template = student_template.format(
        student_code=student_code,
        long_form_hint=long_form_hint,
    )
    return filled_student_template


def extract_hints(response: str) -> Tuple[str, str]:
    # Find the start of the long-form hint
    start_index = response.find("Long-form hint:") + len("Long-form hint:")

    # Find the start of the short-form hint (which marks the end of the long-form hint)
    end_index = response.find("Short-form hint:")

    # Extract and strip any leading/trailing whitespace
    long_form_hint = response[start_index:end_index].strip()

    start_index = response.find("Short-form hint:") + len("Short-form hint:")
    short_form_hint = response[start_index:].strip()
    return long_form_hint, short_form_hint
